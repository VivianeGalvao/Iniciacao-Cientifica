@article{Audet2002,
abstract = {This paper contains a new convergence analysis for the Lewis and Torczon generalized pattern search (GPS) class of methods for unconstrained and linearly constrained optimization. This analysis is motivated by a desire to understand the successful behavior of the algorithm under hypotheses that are satisfied by many practical problems. Specifically, even if the objective function is discontinuous or extended-valued, the methods find a limit point with some minimizing properties. Simple examples show that the strength of the optimality conditions at a limit point depends not only on the algorithm, but also on the directions it uses and on the smoothness of the objective at the limit point in question. The contribution of this paper is to provide a simple convergence analysis that supplies detail about the relation of optimality conditions to objective smoothness properties and to the defining directions for the algorithm, and it gives previous results as corollaries.},
author = {Audet, Charles and Dennis, J. E.},
doi = {10.1137/S1052623400378742},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
language = {en},
month = jan,
number = {3},
pages = {889--903},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Analysis of Generalized Pattern Searches}},
url = {http://epubs.siam.org/doi/abs/10.1137/S1052623400378742},
volume = {13},
year = {2002}
}
@article{Bandeira2012,
author = {Bandeira, A. S. and Scheinberg, K. and Vicente, L. N.},
doi = {10.1007/s10107-012-0578-z},
issn = {0025-5610},
journal = {Mathematical Programming},
month = jul,
number = {1},
pages = {223--257},
title = {{Computation of sparse low degree interpolating polynomials and their application to derivative-free optimization}},
url = {http://link.springer.com/10.1007/s10107-012-0578-z},
volume = {134},
year = {2012}
}
@article{Conn2006,
author = {Conn, A. R. and Scheinberg, K. and Vicente, Lu\'{\i}s N.},
doi = {10.1007/s10107-006-0073-5},
issn = {0025-5610},
journal = {Mathematical Programming},
month = dec,
number = {1-2},
pages = {141--172},
title = {{Geometry of interpolation sets in derivative free optimization}},
url = {http://link.springer.com/10.1007/s10107-006-0073-5},
volume = {111},
year = {2006}
}
@article{Conn2013,
abstract = {We consider a derivative-free optimization, and in particular black box optimization, where the functions to be minimized and the functions representing the constraints are given by black boxes without derivatives. Two fundamental families of methods are available: model-based methods and directional direct search algorithms. This work exploits the flexibility of the second type of methods in order to integrate to a limited extent the models used in the first family. Intensive numerical tests on two sets of 48 and 104 test problems illustrate the efficiency of this hybridization and show that the use of the models improves the performance of the mesh- adaptive direct search algorithm significantly.
We consider a derivative-free optimization, and in particular black box optimization, where the functions to be minimized and the functions representing the constraints are given by black boxes without derivatives. Two fundamental families of methods are available: model-based methods and directional direct search algorithms. This work exploits the flexibility of the second type of methods in order to integrate to a limited extent the models used in the first family. Intensive numerical tests on two sets of 48 and 104 test problems illustrate the efficiency of this hybridization and show that the use of the models improves the performance of the mesh- adaptive direct search algorithm significantly.},
author = {Conn, Andrew R. and {Le Digabel}, S\'{e}bastien},
doi = {10.1080/10556788.2011.623162},
issn = {1055-6788},
journal = {Optimization Methods and Software},
month = feb,
number = {1},
pages = {139--158},
publisher = {Taylor \& Francis},
title = {{Use of quadratic models with mesh-adaptive direct search for constrained black box optimization}},
url = {http://dx.doi.org/10.1080/10556788.2011.623162},
volume = {28},
year = {2013}
}
@article{Custodio2009,
author = {Cust\'{o}dio, A. L. and Rocha, H. and Vicente, L. N.},
doi = {10.1007/s10589-009-9283-0},
issn = {0926-6003},
journal = {Computational Optimization and Applications},
month = aug,
number = {2},
pages = {265--278},
title = {{Incorporating minimum Frobenius norm models inÂ direct search}},
url = {http://link.springer.com/10.1007/s10589-009-9283-0},
volume = {46},
year = {2009}
}
@article{Lucidi2002,
abstract = {In this paper, starting from the study of the common elements that some globally convergent direct search methods share, a general convergence theory is established for unconstrained minimization methods employing only function values. The introduced convergence conditions are useful for developing and analyzing new derivative-free algorithms with guaranteed global convergence. As examples, we describe three new algorithms which combine pattern and line search approaches.},
author = {Lucidi, Stefano and Sciandrone, Marco},
doi = {10.1137/S1052623497330392},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
language = {en},
month = jan,
number = {1},
pages = {97--116},
publisher = {Society for Industrial and Applied Mathematics},
title = {{On the Global Convergence of Derivative-Free Methods for Unconstrained Optimization}},
url = {http://epubs.siam.org/doi/abs/10.1137/S1052623497330392?journalCode=sjope8},
volume = {13},
year = {2002}
}
%esse daqui eu encontrei o artigo
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{Wen2013,
abstract = {An efficient algorithm named Pattern search (PS) has been used widely in various scientific and engineering fields. However, even though the global convergence of PS has been proved, it does not perform well on more complex and higher dimension problems nowadays. In order to improve the efficiency of PS and obtain a more powerful algorithm for global optimization, a new algorithm named Free Pattern Search (FPS) based on PS and Free Search (FS) is proposed in this paper. FPS inherits the global search from FS and the local search from PS. Two operators have been designed for accelerating the convergence speed and keeping the diversity of population. The acceleration operator inspired by FS uses a self-regular management to classify the population into two groups and accelerates all individuals in the first group, while the throw operator is designed to avoid the reduplicative search of population and keep the diversity. In order to verify the performance of FPS, two famous benchmark instances are conducted for the comparisons between FPS with Particle Swarm Optimization (PSO) variants and Differential Evolution (DE) variants. The results show that FPS obtains better solutions and achieves the higher convergence speed than other algorithms.},
author = {Wen, Long and Gao, Liang and Li, Xinyu and Zhang, Liping},
doi = {10.1016/j.asoc.2013.05.004},
file = {:C$\backslash$:/Users/Viviane/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wen et al. - 2013 - Free Pattern Search for global optimization.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
month = sep,
number = {9},
pages = {3853--3863},
title = {{Free Pattern Search for global optimization}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494613001592},
volume = {13},
year = {2013}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{,
language = {en},
title = {{
    
            A derivative free optimization algorithm in practice : Multidisciplinary Analysis Optimization Conferences: Vol. , No. 
        
    (AIAA)
}},
url = {http://arc.aiaa.org/doi/abs/10.2514/6.1998-4718}
}
@misc{,
title = {{SURVEY OF TRUST-REGION DERIVATIVE FREE OPTIMIZATION METHODS}},
url = {http://www.optimization-online.org/DB\_FILE/2007/03/1621.pdf},
urldate = {10/04/14}
}
@article{Audet2014,
abstract = {The mesh adaptive direct search (Mads) class of algorithms is designed for nonsmooth optimization, where the objective function and constraints are typically computed by launching a time-consuming computer simulation. Each iteration of a Mads algorithm attempts to improve the current best-known solution by launching the simulation at a finite number of trial points. Common implementations of Mads generate 2n trial points at each iteration, where \$n\$ is the number of variables in the optimization problem. The objective of the present work is to dynamically reduce that number. We present an algorithmic framework that reduces the number of simulations to exactly \$n+1\$, without impacting the theoretical guarantees from the convergence analysis. Numerical experiments are conducted for several different contexts; the results suggest that these strategies allow the new algorithms to reach a better solution with fewer function evaluations.},
author = {Audet, Charles and Ianni, Andrea and Digabel, S\'{e}bastien Le and Tribes, Christophe},
isbn = {10.1137/120895056},
language = {en},
month = apr,
publisher = {Society for Industrial and Applied Mathematics},
title = {{Reducing the Number of Function Evaluations in Mesh Adaptive Direct Search Algorithms}},
url = {http://epubs.siam.org/doi/abs/10.1137/120895056},
year = {2014}
}
